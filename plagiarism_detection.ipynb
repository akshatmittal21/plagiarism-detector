{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plagiarism Detection using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting docs file into single text\n",
    "def getText(filename):\n",
    "    doc=docx.Document(filename)\n",
    "    fullText=[]\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading .docx file and converting it in list\n",
    "docFiles=[]\n",
    "docNames=[]\n",
    "dirPath='./sample_files'\n",
    "for filename in os.listdir(dirPath): # Add path for the files\n",
    "    if filename.endswith('.docx'):\n",
    "        docNames.append(filename)\n",
    "        filename=getText(dirPath+'/'+filename)\n",
    "        docFiles.append(filename)\n",
    "# docFiles.sort(key=str.lower)\n",
    "# docNames.sort(key=str.lower)\n",
    "\n",
    "print(len(docFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary from the words in all docs\n",
    "def build_lexicon(corpus):\n",
    "    lexicon=set()\n",
    "    for doc in corpus:\n",
    "        word_token=[word for word in doc.split()]\n",
    "        lower_word_list=[i.lower() for i in word_token]\n",
    "        \n",
    "        porter=nltk.PorterStemmer()\n",
    "        stemmed_word=[porter.stem(t) for t in lower_word_list]\n",
    "        \n",
    "        stop_words=set(stopwords.words('english'))\n",
    "        filtered_bag_of_word=[w for w in stemmed_word if not w in stop_words]\n",
    "        lexicon.update(filtered_bag_of_word)\n",
    "    return lexicon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=build_lexicon(docFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating term frequency : Total occurence of words from vocabulary in each doc\n",
    "def tf(term,document):\n",
    "    return freq(term,document)\n",
    "\n",
    "def freq(term,document):\n",
    "    return document.split().count(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix=[]\n",
    "print('\\nOur vocabulary vector is ['+', '.join(list(vocabulary))+']')\n",
    "for doc in docFiles:\n",
    "    tf_vector=[tf(word,doc) for word in vocabulary]\n",
    "    tf_vector_string=', '.join(format(freq,'d') for freq in tf_vector)\n",
    "    print('\\nThe tf vector for Document %d is [%s]' % ((docFiles.index(doc)+1),tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "\n",
    "print('\\nAll combined, here is our master document tree matrix: ')\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the values so that the square of each value is not greater than 1\n",
    "def l2_normalizer(vec):\n",
    "    denom=np.sum([el**2 for el in vec])\n",
    "    return [(el/math.sqrt(denom)) for el in vec]\n",
    "\n",
    "doc_term_matrix_l2=[]\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "    \n",
    "print('\\nA regular old document term matrix: ')\n",
    "print(np.matrix(doc_term_matrix))\n",
    "print('\\nA document term matrix with row-wise L2 norms of 1: ')\n",
    "print(np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating idf - inverse document frequency\n",
    "def numDocsContaining(word,doclist):\n",
    "    document=0\n",
    "    for doc in doclist:\n",
    "        if freq(word,doc)>0:\n",
    "            document+=1\n",
    "    return document\n",
    "\n",
    "def idf(word,doclist):\n",
    "    n_samples=len(doclist)\n",
    "    df=numDocsContaining(word,doclist)\n",
    "    return np.log(n_samples/1+df)\n",
    "\n",
    "my_idf_vector=[idf(word,docFiles) for word in vocabulary]\n",
    "\n",
    "print('\\nOur vocabulary vector is ['+', '.join(list(vocabulary))+']')\n",
    "print('\\nThe inverse document frequency vector is ['+', '.join(format(freq,'f') for freq in my_idf_vector)+']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a diagonal matrix from idf\n",
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat=np.zeros((len(idf_vector),len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat,idf_vector)\n",
    "    return idf_mat\n",
    "\n",
    "my_idf_matrix=build_idf_matrix(my_idf_vector)\n",
    "print('\\nidf matrix is: ')\n",
    "print(my_idf_matrix)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating tfidf matrix\n",
    "doc_term_matrix_tfidf=[]\n",
    "\n",
    "for tf_vector in doc_term_matrix:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector,my_idf_matrix))\n",
    "    \n",
    "doc_term_matrix_tfidf_l2=[]\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "\n",
    "print(vocabulary)\n",
    "print(np.matrix(doc_term_matrix_tfidf_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating plagiarism percentage using cosine similarity\n",
    "for i in range (len(docFiles)):\n",
    "    for j in range(i+1,len(docFiles)):\n",
    "        result_nltk=nltk.cluster.util.cosine_distance(doc_term_matrix_tfidf_l2[i],doc_term_matrix_tfidf_l2[j])\n",
    "        print('\\nComparison between %s and %s: '%(docNames[i],docNames[j]))\n",
    "#         print(result_nltk)\n",
    "        cos_sin=1-result_nltk;\n",
    "        angle_in_radians=math.acos(cos_sin)\n",
    "        \n",
    "        plagiarism=int(cos_sin*100)\n",
    "        print('plagiarism=%s %%\\n'% plagiarism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
